{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c77da2f-1356-417b-be28-c3eaad7a7011",
   "metadata": {},
   "source": [
    "<p style=\"background-color: #0055A4; font-size: 48px; font-weight: bold; color: white; padding: 20px; text-align: center;\">\n",
    "ðŸ§¹ Data Cleaning \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eeccbbd-a357-410b-a8de-883d94a36657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as stats\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import skew, shapiro\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from prettytable import PrettyTable\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e5ec07-2c78-408b-8356-d3788a5a61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "train_df = pd.read_csv('C:/Users/Mega-Pc/Freelance/data/fraudTrain.csv', delimiter=',', encoding='utf-8',index_col=0)\n",
    "test_df = pd.read_csv('C:/Users/Mega-Pc/Freelance/data/fraudTest.csv', delimiter=',', encoding='utf-8',index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a853c7-cf5f-4ea0-9943-d66d33a84a41",
   "metadata": {},
   "source": [
    "## ðŸ”„ Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1ab848-1386-4332-a353-44543eff0e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records: 0\n"
     ]
    }
   ],
   "source": [
    "#train_df\n",
    "duplicate_count = train_df.duplicated().sum()\n",
    "print(f'Number of duplicate records: {duplicate_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9d198e-2a67-4682-b208-d6f02cae58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records: 0\n"
     ]
    }
   ],
   "source": [
    "#train_df\n",
    "duplicate_count = test_df.duplicated().sum()\n",
    "print(f'Number of duplicate records: {duplicate_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88f733-0e4d-4857-9747-91a108f2e4bc",
   "metadata": {},
   "source": [
    "## âœ¨ Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f26df6-e67c-4ebc-9fff-8a787293e886",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b27ee5-d376-446b-a63b-aa5f2c91691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df + test_df\n",
    "skewed_cols = ['amt','city_pop']\n",
    "for col in skewed_cols:\n",
    "    train_df[col] = np.log1p(train_df[col])  \n",
    "    test_df[col] = np.log1p(test_df[col])\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb65409-e551-4ff2-bb14-e91f326eb822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amt</td>\n",
       "      <td>-0.298852</td>\n",
       "      <td>-0.527247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city_pop</td>\n",
       "      <td>0.606094</td>\n",
       "      <td>-0.301258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  Skewness  Kurtosis\n",
       "0       amt -0.298852 -0.527247\n",
       "1  city_pop  0.606094 -0.301258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for var in  skewed_cols:\n",
    "    skewness = stats.skew(train_df[var])\n",
    "    kurtosis = stats.kurtosis(train_df[var])\n",
    "    \n",
    "    results.append({\n",
    "        'Feature':  var ,\n",
    "        'Skewness': skewness,\n",
    "        'Kurtosis': kurtosis\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a81ba-fa90-4190-94cc-273b77091610",
   "metadata": {},
   "source": [
    "## Handle outliers by capping them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37972283-18ab-4897-b234-78984b817833",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars_filtered = [\n",
    "    \"amt\",\n",
    "    \"zip\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"city_pop\",\n",
    "    \"unix_time\",\n",
    "    \"merch_lat\",\n",
    "    \"merch_long\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5809721-0468-4e5b-a5dc-7e4ca9de817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature   Lower Bound   Upper Bound  Outliers Count  Total Points\n",
      "0         amt -7.348234e-01  7.532865e+00             818       1296675\n",
      "1         zip -4.247050e+04  1.407495e+05               0       1296675\n",
      "2         lat  2.364065e+01  5.292025e+01            4679       1296675\n",
      "3        long -1.217580e+02 -5.519800e+01           49922       1296675\n",
      "4    city_pop  1.650397e+00  1.488145e+01            4168       1296675\n",
      "5   unix_time  1.307799e+09  1.390337e+09               0       1296675\n",
      "6   merch_lat  2.389818e+01  5.279255e+01            4967       1296675\n",
      "7  merch_long -1.218880e+02 -5.524608e+01           41994       1296675\n"
     ]
    }
   ],
   "source": [
    "#train_df\n",
    "outliers_info = []\n",
    "\n",
    "for col in numerical_vars_filtered:\n",
    "    Q1 = train_df[col].quantile(0.25)\n",
    "    Q3 = train_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = train_df[(train_df[col] < lower_bound) | (train_df[col] > upper_bound)]\n",
    "    \n",
    "    outliers_info.append({\n",
    "        'Feature': col,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound,\n",
    "        'Outliers Count': len(outliers),\n",
    "        'Total Points': len(train_df)\n",
    "    })\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_info)\n",
    "print(outliers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b89a0a-46ac-411c-aa7c-471fb4097ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "columns_with_outliers = [\n",
    "    \"amt\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"city_pop\",\n",
    "    \"merch_lat\",\n",
    "    \"merch_long\"\n",
    "]\n",
    "\n",
    "for col in columns_with_outliers:\n",
    "    train_df = handle_outliers(train_df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ce4f36-8e4e-4b75-a858-151f127f27ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature   Lower Bound   Upper Bound  Outliers Count  Total Points\n",
      "0         amt -7.348234e-01  7.532865e+00               0       1296675\n",
      "1         zip -4.247050e+04  1.407495e+05               0       1296675\n",
      "2         lat  2.364065e+01  5.292025e+01               0       1296675\n",
      "3        long -1.217580e+02 -5.519800e+01               0       1296675\n",
      "4    city_pop  1.650397e+00  1.488145e+01               0       1296675\n",
      "5   unix_time  1.307799e+09  1.390337e+09               0       1296675\n",
      "6   merch_lat  2.389818e+01  5.279255e+01               0       1296675\n",
      "7  merch_long -1.218880e+02 -5.524608e+01               0       1296675\n"
     ]
    }
   ],
   "source": [
    "# results after capping outliers\n",
    "outliers_info = []\n",
    "\n",
    "for col in numerical_vars_filtered:\n",
    "    Q1 = train_df[col].quantile(0.25)\n",
    "    Q3 = train_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = train_df[(train_df[col] < lower_bound) | (train_df[col] > upper_bound)]\n",
    "    \n",
    "    outliers_info.append({\n",
    "        'Feature': col,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound,\n",
    "        'Outliers Count': len(outliers),\n",
    "        'Total Points': len(train_df)\n",
    "    })\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_info)\n",
    "print(outliers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6714c6-349d-42b6-bd99-59611ffd6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature   Lower Bound   Upper Bound  Outliers Count  Total Points\n",
      "0         amt -7.372033e-01  7.531819e+00             402       1296675\n",
      "1         zip -4.228650e+04  1.405895e+05               0       1296675\n",
      "2         lat  2.383005e+01  5.273365e+01            1933       1296675\n",
      "3        long -1.217322e+02 -5.524100e+01           21104       1296675\n",
      "4    city_pop  1.691879e+00  1.480513e+01            1697       1296675\n",
      "5   unix_time  1.361271e+09  1.400625e+09               0       1296675\n",
      "6   merch_lat  2.395701e+01  5.275246e+01            2090       1296675\n",
      "7  merch_long -1.218659e+02 -5.530390e+01           17926       1296675\n"
     ]
    }
   ],
   "source": [
    "#test_df\n",
    "outliers_info = []\n",
    "\n",
    "for col in numerical_vars_filtered:\n",
    "    Q1 = test_df[col].quantile(0.25)\n",
    "    Q3 = test_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = test_df[(test_df[col] < lower_bound) | (test_df[col] > upper_bound)]\n",
    "    \n",
    "    outliers_info.append({\n",
    "        'Feature': col,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound,\n",
    "        'Outliers Count': len(outliers),\n",
    "        'Total Points': len(train_df)\n",
    "    })\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_info)\n",
    "print(outliers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b25697-197f-4920-834f-d967dca90328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "columns_with_outliers = [\n",
    "    \"amt\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"city_pop\",\n",
    "    \"merch_lat\",\n",
    "    \"merch_long\"\n",
    "]\n",
    "\n",
    "for col in columns_with_outliers:\n",
    "    train_df = handle_outliers(test_df, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b52c2b-b59e-403f-90fa-4a5062165820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature   Lower Bound   Upper Bound  Outliers Count  Total Points\n",
      "0         amt -7.372033e-01  7.531819e+00               0        555719\n",
      "1         zip -4.228650e+04  1.405895e+05               0        555719\n",
      "2         lat  2.383005e+01  5.273365e+01               0        555719\n",
      "3        long -1.217322e+02 -5.524100e+01               0        555719\n",
      "4    city_pop  1.691879e+00  1.480513e+01               0        555719\n",
      "5   unix_time  1.361271e+09  1.400625e+09               0        555719\n",
      "6   merch_lat  2.395701e+01  5.275246e+01               0        555719\n",
      "7  merch_long -1.218659e+02 -5.530390e+01               0        555719\n"
     ]
    }
   ],
   "source": [
    "# results after capping outliers\n",
    "outliers_info = []\n",
    "\n",
    "for col in numerical_vars_filtered:\n",
    "    Q1 = test_df[col].quantile(0.25)\n",
    "    Q3 = test_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = test_df[(test_df[col] < lower_bound) | (test_df[col] > upper_bound)]\n",
    "    \n",
    "    outliers_info.append({\n",
    "        'Feature': col,\n",
    "        'Lower Bound': lower_bound,\n",
    "        'Upper Bound': upper_bound,\n",
    "        'Outliers Count': len(outliers),\n",
    "        'Total Points': len(train_df)\n",
    "    })\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_info)\n",
    "print(outliers_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39330ae-77c0-4bff-8f4e-702e2eb6bf70",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008ac104-6be9-4dbe-ae12-e1be604d035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frauds per cc number\n",
    "\n",
    "fraud_counts = train_df.groupby(\"cc_num\")[\"is_fraud\"].sum().reset_index()\n",
    "fraud_counts.rename(columns={\"is_fraud\": \"total_fraud\"}, inplace=True)\n",
    "\n",
    "train_df = train_df.merge(fraud_counts, on=\"cc_num\", how=\"left\")\n",
    "\n",
    "test_df = test_df.merge(fraud_counts, on=\"cc_num\", how=\"left\")\n",
    "\n",
    "test_df[\"total_fraud\"].fillna(0, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf37b837-e7ae-41c4-a748-4e90d7f59cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frauds per gender\n",
    "\n",
    "fraud_by_gender = train_df.groupby(\"gender\")[\"is_fraud\"].sum().reset_index()\n",
    "fraud_by_gender.rename(columns={\"is_fraud\": \"fraud_count_gender\"}, inplace=True)\n",
    "\n",
    "train_df = train_df.merge(fraud_by_gender, on=\"gender\", how=\"left\")\n",
    "test_df = test_df.merge(fraud_by_gender, on=\"gender\", how=\"left\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "893ebb81-b098-4d64-bcaf-c9194ca867ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frauds per merchant \n",
    "fraud_by_merchant = train_df.groupby(\"merchant\")[\"is_fraud\"].sum().reset_index()\n",
    "fraud_by_merchant.rename(columns={\"is_fraud\": \"fraud_count_merchant\"}, inplace=True)\n",
    "\n",
    "train_df = train_df.merge(fraud_by_merchant, on=\"merchant\", how=\"left\")\n",
    "test_df = test_df.merge(fraud_by_merchant, on=\"merchant\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe2af806-dfec-4740-8055-d07e8c580345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frauds per category\n",
    "fraud_by_category = train_df.groupby(\"category\")[\"is_fraud\"].sum().reset_index()\n",
    "fraud_by_category.rename(columns={\"is_fraud\": \"fraud_count_category\"}, inplace=True)\n",
    "\n",
    "train_df = train_df.merge(fraud_by_category, on=\"category\", how=\"left\")\n",
    "test_df = test_df.merge(fraud_by_category, on=\"category\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbfa78f0-b3b1-48e7-b54f-160d19c52090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frauds per job\n",
    "fraud_by_job = train_df.groupby(\"job\")[\"is_fraud\"].sum().reset_index()\n",
    "fraud_by_job.rename(columns={\"is_fraud\": \"fraud_count_job\"}, inplace=True)\n",
    "\n",
    "train_df = train_df.merge(fraud_by_job, on=\"job\", how=\"left\")\n",
    "test_df = test_df.merge(fraud_by_job, on=\"job\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331f9e3a-e210-4bf4-bf42-4cfdf203ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transacton amount vs category\n",
    "train_df[\"amt_per_category\"] = train_df.groupby(\"category\")[\"amt\"].transform(lambda x: x / x.mean())\n",
    "test_df[\"amt_per_category\"] = test_df.groupby(\"category\")[\"amt\"].transform(lambda x: x / x.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "210012c5-487c-47bd-9bce-3c6d3c9bf925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract age\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"dob\"] = pd.to_datetime(df[\"dob\"], errors=\"coerce\")  \n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"dob_year\"] = df[\"dob\"].dt.year\n",
    "    df[\"dob_month\"] = df[\"dob\"].dt.month\n",
    "    df[\"dob_day\"] = df[\"dob\"].dt.day\n",
    "\n",
    "from datetime import datetime\n",
    "def add_age_feature(df):\n",
    "    df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "    today_year = datetime.today().year\n",
    "    df['age'] = today_year - df['dob'].dt.year\n",
    "\n",
    "    df['age_group'] = pd.cut(\n",
    "        df['age'],\n",
    "        bins=[0, 25, 40, 60, 100],\n",
    "        labels=['young', 'adult', 'mature', 'senior'] \n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_age_feature(train_df)\n",
    "test_df = add_age_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d851ecf-2b0d-4cd4-b4ad-d2bf78642aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction amount per city pop and categorization into bins\n",
    "def add_transaction_features(df):\n",
    "    \n",
    "    #Transaction amount per city pop\n",
    "    df[\"amt_per_capita\"] = df[\"amt\"] / (df[\"city_pop\"] + 1)  \n",
    "    \n",
    "    #Transaction amount bins \n",
    "    df[\"amt_category\"] = pd.qcut(df[\"amt\"], q=3, labels=[\"low\", \"medium\", \"high\"]) \n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df = add_transaction_features(train_df)\n",
    "test_df = add_transaction_features(test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94b0a3a9-2819-4ec6-9740-e97ccc643511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between customer and merchant (3 methods)\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "   \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371  \n",
    "    return c * r\n",
    "\n",
    "def add_distance_feature(df):\n",
    "    df[\"haversine_dist\"] = haversine_distance(\n",
    "        df[\"lat\"], df[\"long\"], df[\"merch_lat\"], df[\"merch_long\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_distance_feature(train_df)\n",
    "test_df = add_distance_feature(test_df)\n",
    "\n",
    "# interaction feature \n",
    "train_df[\"fraud_distance_interaction\"] = train_df[\"haversine_dist\"] * train_df[\"is_fraud\"]\n",
    "test_df[\"fraud_distance_interaction\"] = test_df[\"haversine_dist\"] * test_df[\"is_fraud\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c616a1-f19c-4cf5-ad72-0448d9d8412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud check per distance\n",
    "def add_distance_risk(df):\n",
    "    df[\"distance_risk\"] = pd.cut(\n",
    "        df[\"haversine_dist\"],\n",
    "        bins=[-1, 5, 50, 500, np.inf],  # adjust thresholds\n",
    "        labels=[\"very_close\", \"close\", \"medium\", \"far\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = add_distance_risk(train_df)\n",
    "test_df = add_distance_risk(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be480e2f-607d-45da-91d1-04cb322aaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction: haversine_dist Ã— amt\n",
    "train_df['haversine_dist_x_amt'] = train_df['haversine_dist'] * train_df['amt']\n",
    "test_df['haversine_dist_x_amt'] = test_df['haversine_dist'] * test_df['amt']\n",
    "\n",
    "# Interaction: haversine_dist Ã— city_pop\n",
    "train_df['haversine_dist_x_city_pop'] = train_df['haversine_dist'] * train_df['city_pop']\n",
    "test_df['haversine_dist_x_city_pop'] = test_df['haversine_dist'] * test_df['city_pop']\n",
    "\n",
    "# Interaction: gender Ã— category\n",
    "for df in [train_df, test_df]:\n",
    "    if df['gender'].dtype == 'object':\n",
    "        df['gender_code'] = df['gender'].astype('category').cat.codes\n",
    "    if df['category'].dtype == 'object':\n",
    "        df['category_code'] = df['category'].astype('category').cat.codes\n",
    "    df['gender_x_category'] = df['gender_code'] * df['category_code']\n",
    "\n",
    "# Interaction: job Ã— amt\n",
    "for df in [train_df, test_df]:\n",
    "    if df['job'].dtype == 'object':\n",
    "        df['job_code'] = df['job'].astype('category').cat.codes\n",
    "    df['job_x_amt'] = df['job_code'] * df['amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e97773d-4a82-4687-982f-036d3229ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer level aggregation features \n",
    "def add_customer_agg_features(train_df, test_df):\n",
    "    customer_stats = train_df.groupby(\"cc_num\").agg(\n",
    "        avg_transaction_amt=(\"amt\", \"mean\"),\n",
    "        std_transaction_amt=(\"amt\", \"std\"),\n",
    "        transaction_count=(\"amt\", \"count\")\n",
    "    ).reset_index()\n",
    "    \n",
    "    train_df = train_df.merge(customer_stats, on=\"cc_num\", how=\"left\")\n",
    "    \n",
    "    test_df = test_df.merge(customer_stats, on=\"cc_num\", how=\"left\")\n",
    "    \n",
    "    train_df[\"std_transaction_amt\"].fillna(0, inplace=True)\n",
    "    test_df[\"std_transaction_amt\"].fillna(0, inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = add_customer_agg_features(train_df, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50e5a1fa-077e-4f1c-a8a0-be54e796024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transaction velocity (1hour/24hours/7days)\n",
    "\n",
    "def add_transaction_velocity(df):\n",
    "    df[\"trans_date_trans_time\"] = pd.to_datetime(df[\"trans_date_trans_time\"])\n",
    "    df = df.sort_values(by=[\"cc_num\", \"trans_date_trans_time\"]).reset_index(drop=True)\n",
    "    \n",
    "    tx_1h = np.zeros(len(df), dtype=int)\n",
    "    tx_24h = np.zeros(len(df), dtype=int)\n",
    "    tx_7d = np.zeros(len(df), dtype=int)\n",
    "    \n",
    "    for cust_id, group in df.groupby(\"cc_num\", sort=False):\n",
    "        idx = group.index.values\n",
    "        times = group[\"trans_date_trans_time\"].values.astype(\"datetime64[ns]\")\n",
    "        times_ns = times.astype(\"int64\")\n",
    "        \n",
    "        one_hour = np.timedelta64(1, \"h\").astype(\"int64\")\n",
    "        one_day = np.timedelta64(1, \"D\").astype(\"int64\")\n",
    "        seven_days = np.timedelta64(7, \"D\").astype(\"int64\")\n",
    "        \n",
    "        for arr, window in [(tx_1h, one_hour), (tx_24h, one_day), (tx_7d, seven_days)]:\n",
    "            left_idx = np.searchsorted(times_ns, times_ns - window, side=\"left\")\n",
    "            count = np.arange(len(times)) - left_idx\n",
    "            arr[idx] = count\n",
    "    \n",
    "    df[\"tx_count_last_1h\"] = tx_1h\n",
    "    df[\"tx_count_last_24h\"] = tx_24h\n",
    "    df[\"tx_count_last_7d\"] = tx_7d\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = add_transaction_velocity(train_df)\n",
    "test_df = add_transaction_velocity(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae496bc4-7777-4302-a38a-64949b5d7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transaction Amount Deviation to capture how unusual a transaction is for a customer\n",
    "\n",
    "#amt_diff_mean=current transaction minus customerâ€™s mean transaction amount\n",
    "#amt_diff_std =current transaction â€“ mean) divided by customerâ€™s std\n",
    "#amt_ratio_mean=ratio of current transaction to customerâ€™s mean\n",
    "\n",
    "def add_amount_deviation_features(train_df, test_df):\n",
    "    customer_stats = train_df.groupby(\"cc_num\")[\"amt\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    customer_stats.rename(columns={\"mean\": \"cust_amt_mean\", \"std\": \"cust_amt_std\"}, inplace=True)\n",
    "    \n",
    "    train_df = train_df.merge(customer_stats, on=\"cc_num\", how=\"left\")\n",
    "    test_df = test_df.merge(customer_stats, on=\"cc_num\", how=\"left\")\n",
    "    \n",
    "    train_df[\"cust_amt_std\"].fillna(0, inplace=True)\n",
    "    test_df[\"cust_amt_std\"].fillna(0, inplace=True)\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        df[\"amt_diff_mean\"] = df[\"amt\"] - df[\"cust_amt_mean\"]\n",
    "        df[\"amt_diff_std\"] = (df[\"amt\"] - df[\"cust_amt_mean\"]) / (df[\"cust_amt_std\"] + 1e-6)\n",
    "        df[\"amt_ratio_mean\"] = df[\"amt\"] / (df[\"cust_amt_mean\"] + 1e-6)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = add_amount_deviation_features(train_df, test_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "695accfd-7c7a-4faa-9074-9408a51e62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merchant-Level Aggregation Features because fraud can target certain merchants or categories\n",
    "\n",
    "#merchant_avg_amt=average transaction amount per merchant\n",
    "#merchant_tx_count=number of transactions per merchant\n",
    "#merchant_fraud_rate =fraud rate per merchant \n",
    "\n",
    "def add_merchant_agg_features(train_df, test_df):\n",
    "    merchant_stats = train_df.groupby(\"merchant\").agg(\n",
    "        merchant_avg_amt=(\"amt\", \"mean\"),\n",
    "        merchant_tx_count=(\"amt\", \"count\"),\n",
    "        merchant_fraud_rate=(\"is_fraud\", \"mean\") \n",
    "    ).reset_index()\n",
    "    \n",
    "    train_df = train_df.merge(merchant_stats, on=\"merchant\", how=\"left\")\n",
    "    test_df = test_df.merge(merchant_stats, on=\"merchant\", how=\"left\")\n",
    "\n",
    "    global_fraud_rate = train_df[\"is_fraud\"].mean()\n",
    "    test_df[\"merchant_fraud_rate\"].fillna(global_fraud_rate, inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = add_merchant_agg_features(train_df, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f95b6f97-df78-4b7c-b0f5-a20f1e4c8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# city fraud rate & state fraud rate\n",
    "\n",
    "def add_geographical_features(train_df, test_df):\n",
    "    state_stats = train_df.groupby(\"state\")[\"is_fraud\"].mean().reset_index()\n",
    "    state_stats.rename(columns={\"is_fraud\": \"state_fraud_rate\"}, inplace=True)\n",
    "\n",
    "    city_stats = train_df.groupby(\"city\")[\"is_fraud\"].mean().reset_index()\n",
    "    city_stats.rename(columns={\"is_fraud\": \"city_fraud_rate\"}, inplace=True)\n",
    "    \n",
    "    train_df = train_df.merge(state_stats, on=\"state\", how=\"left\")\n",
    "    train_df = train_df.merge(city_stats, on=\"city\", how=\"left\")\n",
    "    \n",
    "    test_df = test_df.merge(state_stats, on=\"state\", how=\"left\")\n",
    "    test_df = test_df.merge(city_stats, on=\"city\", how=\"left\")\n",
    "    \n",
    "    global_fraud_rate = train_df[\"is_fraud\"].mean()\n",
    "    test_df[\"state_fraud_rate\"].fillna(global_fraud_rate, inplace=True)\n",
    "    test_df[\"city_fraud_rate\"].fillna(global_fraud_rate, inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = add_geographical_features(train_df, test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84a26f63-6781-4f32-b64b-35154c3263d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit card number related features \n",
    "\n",
    "# average transaction ampunt per card\n",
    "cc_mean_amt = train_df.groupby(\"cc_num\")[\"amt\"].mean()\n",
    "train_df[\"cc_avg_amt\"] = train_df[\"cc_num\"].map(cc_mean_amt)\n",
    "test_df[\"cc_avg_amt\"] = test_df[\"cc_num\"].map(cc_mean_amt)\n",
    "\n",
    "global_mean = train_df[\"amt\"].mean()\n",
    "test_df[\"cc_avg_amt\"].fillna(global_mean, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b28fbcf9-bb9d-4eb3-b3d1-194b323116c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud rate per card \n",
    "\n",
    "cc_fraud_rate = train_df.groupby(\"cc_num\")[\"is_fraud\"].mean()\n",
    "train_df[\"cc_fraud_rate\"] = train_df[\"cc_num\"].map(cc_fraud_rate)\n",
    "test_df[\"cc_fraud_rate\"] = test_df[\"cc_num\"].map(cc_fraud_rate)\n",
    "\n",
    "global_fraud_rate = train_df[\"is_fraud\"].mean()\n",
    "test_df[\"cc_fraud_rate\"].fillna(global_fraud_rate, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "240b63a0-71d7-4840-901a-958f323b85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of transcations per cc in a recent time window\n",
    "def add_transaction_velocity(df):\n",
    "    df[\"trans_date_trans_time\"] = pd.to_datetime(df[\"trans_date_trans_time\"])\n",
    "    df = df.sort_values(by=[\"cc_num\", \"trans_date_trans_time\"])\n",
    "    \n",
    "    df[\"tx_count_last_1h\"] = 0\n",
    "    df[\"tx_count_last_24h\"] = 0\n",
    "    df[\"tx_count_last_7d\"] = 0\n",
    "    \n",
    "    for cc in df[\"cc_num\"].unique():\n",
    "        cc_df = df[df[\"cc_num\"] == cc]\n",
    "        cc_df = cc_df.set_index(\"trans_date_trans_time\")\n",
    "        \n",
    "        # Rolling counts\n",
    "        df.loc[cc_df.index, \"tx_count_last_1h\"] = cc_df.rolling(\"1h\").count()[\"cc_num\"] - 1\n",
    "        df.loc[cc_df.index, \"tx_count_last_24h\"] = cc_df.rolling(\"1d\").count()[\"cc_num\"] - 1\n",
    "        df.loc[cc_df.index, \"tx_count_last_7d\"] = cc_df.rolling(\"7d\").count()[\"cc_num\"] - 1\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fda16-05b1-4912-a5a1-082b0d9b5dd3",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4def3c14-aea7-49d3-b62d-a57c9c559322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'merchant': {'train_unique': 693, 'test_unique': 693},\n",
       " 'category': {'train_unique': 14, 'test_unique': 14},\n",
       " 'first': {'train_unique': 341, 'test_unique': 341},\n",
       " 'last': {'train_unique': 471, 'test_unique': 471},\n",
       " 'gender': {'train_unique': 2, 'test_unique': 2},\n",
       " 'street': {'train_unique': 924, 'test_unique': 924},\n",
       " 'city': {'train_unique': 849, 'test_unique': 849},\n",
       " 'state': {'train_unique': 50, 'test_unique': 50},\n",
       " 'job': {'train_unique': 478, 'test_unique': 478},\n",
       " 'trans_num': {'train_unique': 555719, 'test_unique': 555719}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = [col for col in train_df.columns if train_df[col].dtype == 'object']\n",
    "\n",
    "unique_counts = {}\n",
    "for col in categorical_columns:\n",
    "    unique_counts[col] = {\n",
    "        \"train_unique\": train_df[col].nunique(),\n",
    "        \"test_unique\": test_df[col].nunique()\n",
    "    }\n",
    "\n",
    "unique_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f621808a-1aac-4f33-bb13-a649b7238590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le_age = LabelEncoder()\n",
    "le_amt = LabelEncoder()\n",
    "\n",
    "\n",
    "train_df['age_group_encoded'] = le_age.fit_transform(train_df['age_group'])\n",
    "test_df['age_group_encoded'] = le_age.transform(test_df['age_group'])\n",
    "\n",
    "train_df['amt_category_encoded'] = le_amt.fit_transform(train_df['amt_category'])\n",
    "test_df['amt_category_encoded'] = le_amt.transform(test_df['amt_category'])\n",
    "\n",
    "train_df.drop(['age_group', 'amt_category'], axis=1, inplace=True)\n",
    "test_df.drop(['age_group', 'amt_category'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eac54a35-75cb-4cd5-b13b-17314c3f8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-cardinality = One-hot encoding\n",
    "onehot_cols = [\"category\", \"gender\", \"state\"]\n",
    "\n",
    "# High-cardinality = Target encoding\n",
    "target_cols = [\"merchant\", \"first\", \"last\", \"street\", \"city\", \"job\"]\n",
    "\n",
    "# ---------------------------\n",
    "# One-hot encoding\n",
    "# ---------------------------\n",
    "combined = pd.concat([train_df[onehot_cols], test_df[onehot_cols]], axis=0)\n",
    "combined_onehot = pd.get_dummies(combined, columns=onehot_cols, dtype=float)  \n",
    "\n",
    "train_onehot = combined_onehot.iloc[:len(train_df), :].reset_index(drop=True)\n",
    "test_onehot = combined_onehot.iloc[len(train_df):, :].reset_index(drop=True)\n",
    "\n",
    "train_df = pd.concat([train_df.drop(columns=onehot_cols), train_onehot], axis=1)\n",
    "test_df = pd.concat([test_df.drop(columns=onehot_cols), test_onehot], axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# Target encoding for high-cardinality columns\n",
    "# ---------------------------\n",
    "def target_encode(train, test, col, target=\"is_fraud\"):\n",
    "    mean_map = train.groupby(col)[target].mean()\n",
    "    train_encoded = train[col].map(mean_map)\n",
    "    test_encoded = test[col].map(mean_map)\n",
    "    global_mean = train[target].mean()\n",
    "    test_encoded.fillna(global_mean, inplace=True)\n",
    "    return train_encoded.astype(float), test_encoded.astype(float)  \n",
    "\n",
    "for col in target_cols:\n",
    "    train_df[col], test_df[col] = target_encode(train_df, test_df, col, target=\"is_fraud\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506d8f9-d003-40fd-a9f5-950b19d2ae3f",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eef13c9c-3aa6-4285-acd2-e094d6d78141",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"trans_num\",\"trans_date_trans_time\",\"dob\"]\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bd3c9-d6af-40b2-9de1-83bf015ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col != 'cc_num']\n",
    "\n",
    "# Compute correlation with target\n",
    "target_col = 'is_fraud'\n",
    "features_for_corr = numeric_features.copy()\n",
    "if target_col not in features_for_corr:\n",
    "    features_for_corr.append(target_col)\n",
    "\n",
    "correlation_with_target = train_df[features_for_corr].corr()[target_col].sort_values()\n",
    "\n",
    "# Interactive bar chart\n",
    "pastel_blue_colors = ['#b0d0f3', '#93c9e6', '#7cb9d4', '#6ca3c1', '#4f87a3']\n",
    "\n",
    "fig = px.bar(\n",
    "    x=correlation_with_target.index,\n",
    "    y=correlation_with_target.values,\n",
    "    labels={\"x\": \"Features\", \"y\": \"Pearson Correlation Coefficient\"},\n",
    "    title=f\"Correlation of Numeric Features with Target ({target_col})\",\n",
    "    color=correlation_with_target.values,\n",
    "    color_continuous_scale=pastel_blue_colors\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Features\", tickangle=45),\n",
    "    yaxis=dict(title=\"Pearson Correlation Coefficient\"),\n",
    "    coloraxis_colorbar=dict(title=\"Correlation Coefficient\"),\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dd445-2602-460b-8d32-81aba392c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['cc_num']]\n",
    "\n",
    "corr_matrix = train_df[numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, annot_kws={\"size\": 10})\n",
    "plt.title(\"Heatmap of Feature Correlations\")\n",
    "plt.show()\n",
    "\n",
    "positive_corr = []\n",
    "negative_corr = []\n",
    "\n",
    "for row in corr_matrix.columns:\n",
    "    for col in corr_matrix.columns:\n",
    "        if row != col:\n",
    "            if corr_matrix.loc[row, col] > 0.9:\n",
    "                positive_corr.append((row, col, corr_matrix.loc[row, col]))\n",
    "            elif corr_matrix.loc[row, col] < -0.9:\n",
    "                negative_corr.append((row, col, corr_matrix.loc[row, col]))\n",
    "\n",
    "if positive_corr:\n",
    "    print(\"These features are positively correlated (>|0.9|):\")\n",
    "    for pair in positive_corr:\n",
    "        print(f\"{pair[0]} and {pair[1]}: {pair[2]:.2f}\")\n",
    "else:\n",
    "    print(\"No positive correlations (>|0.9|) found.\")\n",
    "\n",
    "if negative_corr:\n",
    "    print(\"\\nThese features are negatively correlated (<-0.9):\")\n",
    "    for pair in negative_corr:\n",
    "        print(f\"{pair[0]} and {pair[1]}: {pair[2]:.2f}\")\n",
    "else:\n",
    "    print(\"No negative correlations (<-0.9) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81a638-c167-4c31-9062-07fa806801fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "numeric_cols = train_df.select_dtypes(include=[\"number\"]).columns.drop(\"is_fraud\")\n",
    "variances = train_df[numeric_cols].var()\n",
    "variance_table = [(col, variance) for col, variance in variances.items()]\n",
    "print(\"Variance of each numeric feature:\")\n",
    "print(tabulate(variance_table, headers=[\"Feature\", \"Variance\"], tablefmt=\"pretty\"))\n",
    "\n",
    "low_variance_features = variances[variances < 0.01].index.tolist()\n",
    "\n",
    "low_variance_table = [(col, variances[col]) for col in low_variance_features]\n",
    "print(\"\\nLow variance numeric features (variance < 0.01):\")\n",
    "print(tabulate(low_variance_table, headers=[\"Feature\", \"Variance\"], tablefmt=\"pretty\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02def40d-c6a7-49ae-af4d-c910c2b87192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop low  variance features\n",
    "train_df.drop(columns=low_variance_features, inplace=True)\n",
    "test_df.drop(columns=low_variance_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d3d3b-f814-48de-9000-4a7c9f7b5d1f",
   "metadata": {},
   "source": [
    "## Chi Squared Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4a4fe-71d7-48b5-91ad-5522b023fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['is_fraud'])  \n",
    "y_train = train_df['is_fraud']  \n",
    "\n",
    "X_test = test_df.drop(columns=['is_fraud'])  \n",
    "y_test = test_df['is_fraud']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01db71e-ab55-46db-93eb-d3b77fc69b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [col for col in train_df.columns if col == 'is_fraud']\n",
    "col = list(train_df.columns)\n",
    "categorical_features = []\n",
    "discrete_features = []\n",
    "for i in col:\n",
    "    if len(train_df[i].unique()) > 6:\n",
    "        discrete_features.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "light_blue = '\\033[94m'\n",
    "reset_color = '\\033[0m'\n",
    "\n",
    "print(f\"{light_blue}Categorical features ({len(categorical_features)}):{reset_color}\", *categorical_features)\n",
    "print(f\"{light_blue}Target Variable ({len(target)}):{reset_color}\", target)\n",
    "print(f\"{light_blue}Discrete features ({len(discrete_features)}):{reset_color}\", *discrete_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171ec87-ab4d-4dc0-8f47-3173007def74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "colors = ['#3C1053', '#DF6589']\n",
    "flag_columns = [col for col in categorical_features if col not in ['is_fraud']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "features = X_train.loc[:, flag_columns].clip(lower=0) \n",
    "target = y_train\n",
    "\n",
    "best_features = SelectKBest(score_func=chi2, k='all')\n",
    "fit = best_features.fit(features, target)\n",
    "\n",
    "featureScores_train = pd.DataFrame(data=fit.scores_, index=features.columns, columns=['Chi Squared Score'])\n",
    "sorted_featureScores_train = featureScores_train.sort_values(ascending=False, by='Chi Squared Score')\n",
    "\n",
    "sns.heatmap(sorted_featureScores_train, annot=True, cmap=colors, linewidths=0.4, linecolor='black', fmt='.2f')\n",
    "plt.title('Selection of Categorical Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f91811-c507-4c13-93fc-dad140d02e05",
   "metadata": {},
   "source": [
    "## ANOVA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c6817-0619-4c37-bd66-e2351fb3e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "colors = ['#3C1053', '#DF6589']\n",
    "discrete_columns = [col for col in discrete_features if col not in ['cc_num']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10)) \n",
    "\n",
    "features = X_train.loc[:, discrete_columns]\n",
    "target = y_train\n",
    "\n",
    "best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "fit = best_features.fit(features, target)\n",
    "\n",
    "featureScores_train = pd.DataFrame(data=fit.scores_, index=list(features.columns), columns=['ANOVA Score'])\n",
    "sorted_featureScores_train = featureScores_train.sort_values(ascending=False, by='ANOVA Score')\n",
    "\n",
    "sns.heatmap(sorted_featureScores_train, annot=True, cmap=colors, linewidths=0.4, linecolor='black', fmt='.2f', annot_kws={'size': 10})\n",
    "plt.title('Selection of Numerical Features ')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a126e0-e97d-437e-9a35-9891e1e16d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop feattures that have p_value>0.05 in anova test\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "discrete_features = []\n",
    "for i in col:\n",
    "    if len(train_df[i].unique()) > 6:\n",
    "        discrete_features.append(i)\n",
    "best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "fit = best_features.fit(X_train.loc[:, discrete_columns], y_train)\n",
    "\n",
    "p_values_anova = best_features.pvalues_\n",
    "unimportant_features_anova = [X_train.columns[i] for i in range(len(p_values_anova)) if p_values_anova[i] > 0.05]\n",
    "\n",
    "print(\"Unimportant features (p-value > 0.05):\")\n",
    "print(unimportant_features_anova)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee3d31-f56c-4b90-95ef-80bf0dc63133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=unimportant_features_anova)\n",
    "X_test = X_test.drop(columns=unimportant_features_anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f7254-624f-451e-8861-3d855213e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "columns_to_consider = [col for col in train_df.columns if col not in ['cc_num']]\n",
    "X = train_df[columns_to_consider]\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Filter features with high VIF\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 15][\"Feature\"].tolist()\n",
    "\n",
    "# Compute ANOVA F-statistic\n",
    "best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "fit = best_features.fit(X, train_df['is_fraud'])  \n",
    "\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'ANOVA F-Statistic': fit.scores_\n",
    "})\n",
    "\n",
    "# Generate pairwise correlations \n",
    "corr_matrix = X[high_vif_features].corr()\n",
    "high_corr_pairs = []\n",
    "processed = set()\n",
    "\n",
    "for row in high_vif_features:\n",
    "    for col in high_vif_features:\n",
    "        if row != col and (col, row) not in processed:\n",
    "            processed.add((row, col))\n",
    "            corr_val = corr_matrix.loc[row, col]\n",
    "            if abs(corr_val) > 0.95:\n",
    "                high_corr_pairs.append((row, col, corr_val))\n",
    "\n",
    "features_to_delete = []\n",
    "\n",
    "for feature1, feature2, _ in high_corr_pairs:\n",
    "    f_stat1 = feature_scores.loc[feature_scores['Feature'] == feature1, 'ANOVA F-Statistic'].values[0]\n",
    "    f_stat2 = feature_scores.loc[feature_scores['Feature'] == feature2, 'ANOVA F-Statistic'].values[0]\n",
    "    \n",
    "    if f_stat1 < f_stat2:\n",
    "        features_to_delete.append(feature1)\n",
    "    else:\n",
    "        features_to_delete.append(feature2)\n",
    "\n",
    "features_to_delete = list(set(features_to_delete))  \n",
    "\n",
    "print(\"\\nðŸ—‘ï¸ Features to delete due to high VIF and low ANOVA F-statistic:\")\n",
    "print(features_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc42271-7264-4cf1-9f9f-5459d47bc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_delete=['amt_diff_mean', 'cust_amt_mean', 'cust_amt_std', 'cc_avg_amt', 'gender_F', 'euclidean_dist', 'dob_year',  'amt_diff_std']\n",
    "X_train = X_train.drop(columns=features_to_delete)\n",
    "X_test= X_test.drop(columns=features_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f441a1-d815-4ee0-9908-46ba8e4742cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4694d-4f5b-4305-9afc-48f02399605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_train = train_df.corr()\n",
    "correlation_with_target_train = correlation_matrix_train['is_fraud']\n",
    "\n",
    "low_correlation_features = correlation_with_target_train[abs(correlation_with_target_train) < 0.1].index.tolist()\n",
    "low_correlation_features = [feature for feature in low_correlation_features if feature not in ['month', 'encrypted_msisdn']]\n",
    "\n",
    "print(\"Features with correlation lower than 0.1 with respect to target 'Fraud':\")\n",
    "print(low_correlation_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42916b-f2b4-4858-bb8d-68678e000880",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "c18fdea5-e79e-421c-a7e9-849cd57e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "exclude_cols = ['cc_num', 'is_fraud']\n",
    "\n",
    "numerical_features = [col for col in train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "                      if col not in exclude_cols]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
    "test_df[numerical_features] = scaler.transform(test_df[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "efdebd72-9dcf-4cf4-8532-68abe61225e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>zip</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>dob_month</th>\n",
       "      <th>dob_day</th>\n",
       "      <th>age</th>\n",
       "      <th>avg_transaction_amt</th>\n",
       "      <th>std_transaction_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>state_PA</th>\n",
       "      <th>state_SC</th>\n",
       "      <th>state_TN</th>\n",
       "      <th>state_TX</th>\n",
       "      <th>state_VA</th>\n",
       "      <th>state_WA</th>\n",
       "      <th>state_WI</th>\n",
       "      <th>state_WV</th>\n",
       "      <th>state_WY</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60416207185</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.363638</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.242834</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60416207185</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.363638</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.240082</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60416207185</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.363638</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>0.221548</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60416207185</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.363638</td>\n",
       "      <td>0.005879</td>\n",
       "      <td>0.230414</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60416207185</td>\n",
       "      <td>0.823573</td>\n",
       "      <td>0.363638</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.228224</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>0.129689</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555714</th>\n",
       "      <td>4992346398065154184</td>\n",
       "      <td>0.608915</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>0.602024</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>0.476256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555715</th>\n",
       "      <td>4992346398065154184</td>\n",
       "      <td>0.608915</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.995959</td>\n",
       "      <td>0.582463</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>0.476256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555716</th>\n",
       "      <td>4992346398065154184</td>\n",
       "      <td>0.608915</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.997259</td>\n",
       "      <td>0.597279</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>0.476256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555717</th>\n",
       "      <td>4992346398065154184</td>\n",
       "      <td>0.608915</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.997265</td>\n",
       "      <td>0.577880</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>0.476256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555718</th>\n",
       "      <td>4992346398065154184</td>\n",
       "      <td>0.608915</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.997721</td>\n",
       "      <td>0.597903</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>0.171583</td>\n",
       "      <td>0.476256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>555719 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cc_num       zip  city_pop  unix_time  merch_long  \\\n",
       "0               60416207185  0.823573  0.363638   0.000184    0.242834   \n",
       "1               60416207185  0.823573  0.363638   0.000902    0.240082   \n",
       "2               60416207185  0.823573  0.363638   0.004250    0.221548   \n",
       "3               60416207185  0.823573  0.363638   0.005879    0.230414   \n",
       "4               60416207185  0.823573  0.363638   0.010388    0.228224   \n",
       "...                     ...       ...       ...        ...         ...   \n",
       "555714  4992346398065154184  0.608915  0.266659   0.995300    0.602024   \n",
       "555715  4992346398065154184  0.608915  0.266659   0.995959    0.582463   \n",
       "555716  4992346398065154184  0.608915  0.266659   0.997259    0.597279   \n",
       "555717  4992346398065154184  0.608915  0.266659   0.997265    0.577880   \n",
       "555718  4992346398065154184  0.608915  0.266659   0.997721    0.597903   \n",
       "\n",
       "        dob_month  dob_day  age  avg_transaction_amt  std_transaction_amt  \\\n",
       "0               2       17   39             0.129689             0.493415   \n",
       "1               2       17   39             0.129689             0.493415   \n",
       "2               2       17   39             0.129689             0.493415   \n",
       "3               2       17   39             0.129689             0.493415   \n",
       "4               2       17   39             0.129689             0.493415   \n",
       "...           ...      ...  ...                  ...                  ...   \n",
       "555714          1        9   69             0.171583             0.476256   \n",
       "555715          1        9   69             0.171583             0.476256   \n",
       "555716          1        9   69             0.171583             0.476256   \n",
       "555717          1        9   69             0.171583             0.476256   \n",
       "555718          1        9   69             0.171583             0.476256   \n",
       "\n",
       "        ...  state_PA  state_SC  state_TN  state_TX  state_VA  state_WA  \\\n",
       "0       ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2       ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4       ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "555714  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "555715  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "555716  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "555717  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "555718  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        state_WI  state_WV  state_WY  is_fraud  \n",
       "0            0.0       0.0       1.0         0  \n",
       "1            0.0       0.0       1.0         0  \n",
       "2            0.0       0.0       1.0         0  \n",
       "3            0.0       0.0       1.0         0  \n",
       "4            0.0       0.0       1.0         0  \n",
       "...          ...       ...       ...       ...  \n",
       "555714       0.0       0.0       0.0         0  \n",
       "555715       0.0       0.0       0.0         0  \n",
       "555716       0.0       0.0       0.0         0  \n",
       "555717       0.0       0.0       0.0         0  \n",
       "555718       0.0       0.0       0.0         0  \n",
       "\n",
       "[555719 rows x 67 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee80593-a62c-4205-9681-3b431ef359a8",
   "metadata": {},
   "source": [
    "## Data balancing with SMOTE & ENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b2ffc-c9cb-4e7f-a009-c2e57ad2815a",
   "metadata": {},
   "source": [
    "# ðŸ§ª Data Splitting, SMOTE, and ENN\n",
    "\n",
    "In this step, we prepare the data by **splitting**, **balancing**, and **cleaning** the dataset to ensure fair model training.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‚ **Data Splitting**\n",
    "\n",
    "- **Features (`X`)**: Independent variables.\n",
    "- **Target (`y`)**: Dependent variable, `Churn`.\n",
    "- **Split Ratio**: 80% training, 20% testing.\n",
    "- **Stratification**: Maintains class distribution across splits.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ **SMOTE: Synthetic Minority Oversampling Technique**\n",
    "\n",
    "- **Purpose**: Balances the dataset by generating synthetic samples for the minority class (`Churn=1`).\n",
    "- **How**: Creates new samples by interpolating between existing minority samples.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ **ENN: Edited Nearest Neighbors**\n",
    "\n",
    "- **Purpose**: Removes noisy or borderline samples after oversampling.\n",
    "- **How**: Deletes samples that disagree with the majority of their nearest neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ **Final Outcome**\n",
    "\n",
    "By applying **SMOTE + ENN**, the training dataset becomes balanced, reducing bias and improving model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7a31b-155b-45fd-b835-6a01f4cddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['is_fraud'])  \n",
    "y_train = train_df['is_fraud']  \n",
    "\n",
    "X_test = test_df.drop(columns=['is_fraud'])  \n",
    "y_test = test_df['is_fraud']  \n",
    "\n",
    "# Initialize SMOTE and ENN\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "enn = EditedNearestNeighbours(n_neighbors=11)\n",
    "\n",
    "# Apply SMOTE to balance the minority class\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Apply ENN to clean the data\n",
    "X_res, y_res = enn.fit_resample(X_res, y_res)\n",
    "\n",
    "# Check the class distribution after the first iteration\n",
    "class_distribution = Counter(y_res)\n",
    "print(f\"Class distribution after SMOTE + ENN (1st iteration): {class_distribution}\")\n",
    "\n",
    "# Repeat the process iteratively until classes are balanced\n",
    "while min(class_distribution.values()) != max(class_distribution.values()):\n",
    "    # Apply SMOTE again\n",
    "    X_res, y_res = smote.fit_resample(X_res, y_res)\n",
    "\n",
    "    # Apply ENN to clean the data further\n",
    "    X_res, y_res = enn.fit_resample(X_res, y_res)\n",
    "\n",
    "    # Check the updated class distribution\n",
    "    class_distribution = Counter(y_res)\n",
    "    print(f\"Class distribution after SMOTE + ENN iteration: {class_distribution}\")\n",
    "\n",
    "    # Remove duplicates after each iteration\n",
    "    X_res = pd.DataFrame(X_res).drop_duplicates()\n",
    "    y_res = y_res[X_res.index]\n",
    "\n",
    "# Combine the resampled data into a new balanced DataFrame\n",
    "telecom_combined_Balanced = pd.DataFrame(X_res, columns=X.columns)\n",
    "telecom_combined_Balanced['Churn'] = y_res\n",
    "\n",
    "# Verify the final class distribution\n",
    "print(\"Final class distribution after SMOTE + ENN:\")\n",
    "print(telecom_combined_Balanced['Churn'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae19fb-659c-428c-831f-1c7081fcda9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea468ed-4a8a-4543-bdaf-5f4ec8f32b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3281b7-4580-435b-b4b8-c4afd8fc12ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def6879-3405-49be-ad27-432dcf730bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d19552-6137-4983-a30a-554cad9390e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "2d8b0620-ecd9-4a5a-a90e-eb827121b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df_cleaned.csv', sep=',', encoding='utf-8', index=False)\n",
    "test_df.to_csv('test_df_cleaned.csv', sep=',', encoding='utf-8', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a8878-97bb-4af1-88c5-d92e91beb685",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
